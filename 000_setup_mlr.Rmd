---
title: "tune several models with mlr"
author: "Satoshi Kato"
date: "`r format(Sys.time(), '%Y/%m/%d')`"
output:
  html_document:
    fig_caption: yes
    pandoc_args:
    - --from
    - markdown+autolink_bare_uris+tex_math_single_backslash-implicit_figures
    toc: yes
    toc_depth: 4
    keep_md: yes
  word_document:
    toc: yes
    toc_depth: 3
  pdf_document:
    toc: yes
    toc_depth: 3
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
require(tidyverse)
require(mlr)
require(knitr)
opts_knit$set(progress = TRUE, 
              verbose = TRUE, 
              root.dir = ".")

opts_chunk$set(collapse = FALSE, 
               prompt  = FALSE,
               comment = "", 
               message = TRUE, 
               warning = FALSE, 
               echo=TRUE)
set.seed(12345)
```

# install packages

```{r, eval=FALSE}
install.packages(c("mlr"))
install.packages(c("glmnet", "kernlab", "randomForest", "gbm"))

install.packages(c("DALEX", "iml"), dependencies = TRUE)

```

# create an mlr task and model

regression task for Boston dataset.

```{r mlr.init}
data(apartments, package = "DALEX")
task <- makeRegrTask(id = "ap", data = apartments, target = "m2.price")
tune.ctrl <- makeTuneControlRandom()
res.desc  <- makeResampleDesc("CV", iters = 2)

learner <- NULL
par.set <- NULL

```

# choose model 

```{r listLearners}
listLearners(task) %>%
  select(class, short.name, package)
```

# Model setup 

## linear regression with penalty (elastincnet)

```{r setup.enet}
# getLearnerParamSet(makeLearner("regr.glmnet"))
learner[["enet"]]<- makeLearner("regr.glmnet")
par.set[["enet"]] <- makeParamSet(
  makeNumericParam("alpha", lower = 0, upper = 1),
  makeNumericParam("s",     lower = 1, upper = 10^3))

```

## Support vector machine (SVM) 

```{r setup.svm}
# getLearnerParamSet(makeLearner("regr.ksvm"))
learner[["svm"]]  <- makeLearner("regr.ksvm", kernel = "rbfdot")
par.set[["svm"]]  <- makeParamSet(
  makeNumericParam("C",     lower = -3, upper = 3, trafo = function(x) 10^x),
  makeNumericParam("sigma", lower = -3, upper = 3, trafo = function(x) 10^x))

```

## random forest (RF)

```{r setup.rf}
# getLearnerParamSet(makeLearner("regr.randomForest"))
learner[["rf"]] <- makeLearner("regr.randomForest")
par.set[["rf"]] <- makeParamSet(
  makeIntegerParam("ntree", lower=50, upper=1000))

```

## Gradient Boosting Machine (GBM)

```{r setup.xgb}
# getLearnerParamSet(makeLearner("regr.gbm"))
learner[["gbm"]] <- makeLearner("regr.gbm")
par.set[["gbm"]] <- makeParamSet(
  makeIntegerParam("n.trees",           lower = 3L, upper = 50L),
  makeIntegerParam("interaction.depth", lower = 3L, upper = 20L))

```

# tune model

```{r tune.model, message=FALSE}
model.labels <- names(learner)
# print(model.names)

tuned.par.set <- NULL
tuned.model   <- NULL

for(model.name in model.labels) {
  
  print(model.name)
  tuned.par.set[[model.name]] <- tuneParams(
    learner[[model.name]], 
    task = task, 
    resampling = res.desc,
    par.set = par.set[[model.name]],
    control = tune.ctrl)
  
  # Create a new model using tuned hyperparameters
  tuned.learner <- setHyperPars(
    learner = learner[[model.name]],
    par.vals = tuned.par.set[[model.name]]$x
  )
  
  # Re-train parameters using tuned hyperparameters (and full training set)
  tuned.model[[model.name]] <- train(tuned.learner, task)
  
}


saveRDS(tuned.model, "./tuned_models.RDS")
```

```{r}
for(model.name in model.labels){
  print(model.name)
  print(tuned.model[[model.name]])
}

```

